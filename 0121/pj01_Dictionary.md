# JSON 파일 다루기 (관통프로젝트1)

## A.

> 제공되는 영화 데이터의 주요내용만을 수집하는 문제였다.
>
> `json`파일을 `open `내장함수로 불러오고, 파이썬의 `dict`객체로 다루기 위해  `json.load `함수를 사용하였다.
>
> 사실 그 이후는 dict 객체의 key값만 호출해주면 되는 평이한 문제였다.



## B.

> 위 문제에서 생성한 함수를 호출한 다음, `genre_ids`에 맞는 `genre_names`을 가져와서 붙여주면 되었다.
>
> 사실 어려워 보이긴 하지, 풀이에 있어서 json 파일 하나만 추가된 것이고, `genre_ids`가 리스트라는 것에 주의하여 해당 리스트에 대한 for문을 추가하는 것만 놓치지 않으면 되는 문제였다.
>
> for문을 돌며 해당 키가 일치할 때의 value(name)을 가져오면 된다는 생각이 들었고, 역시 그러했다.
>
> A와 비슷한 난이도였다는 것이 주관적인 생각이다.



번외)

> 개인적으로 생각해볼 때, 이러한 문제해결 (key값이 일치할 때를 찾아 붙이기)은 실제로 많이 일어날 텐데, 분명 merge와 같은 함수 혹은 메소드가 있지 않을까?
>
> 파이썬에는 `pandas `모듈의 `dataframe `구조에 `merge `메소드가 있다.
>
> dict 구조를 `dataframe`으로 바꾸고, `merge `메소드를 적용해야 하니 코드를 짜는 데 있어선 편하겠지만.. 시간척인 측면에서 과연 이게 더 빠를까? 싶었다.
>
> 모듈 호출, 자료구조 변환 등의 절차가 추가되다 보니 많은 메모리와 시간을 잡아먹을 것이다.
>
> 분명 가독성 좋고 편하긴 할 것 같다.



# C.

> 이 문제 역시도 `genre_names`을 가져와야 하기 때문에 B에서 생성한 함수를 이용해주면 되는 문제였다.
>
> B번의 풀이가 있었기에 굉장히 쉬운 문제였다.
>
> for문 안에 B의 함수를 넣어 풀이하였다.



# D.

> 처음 이 문제를 보곤, 문제 자체가 이해되지 않았다...
>
> ~~(30분 정도를 헤맨 것 같다)~~
>
>  revenue 변수가 없는데 자꾸 비교해서 최대값을 찾으라고 한다.
>
> 
>
> 역시 문제를 잘 못 읽은 것이었고, `revenue `변수정보를 가진 json 파일은 각각 영화파일들에 존재하는 것이었다!
>
> 파일 이름이 굉장히 쉽게 `13.json` , `497.json` 등으로 표현되어있었기에
>
> `for-loop` 안에 `f-string`을 이용하여 파일을 하나하나씩 불러오며 풀이하면 되었다.



# E.

> D와 풀이가 굉장히 비슷하여 언급할 소지가 없다.
>
> 문제에 대한 생각보다는 Data에 대한 이야기를 쫌 하면 좋을 것 같다.
>
> 
>
> E 문제를 다 풀고나서 데이터 구조를 살펴보았는데,
>
> `release_date`가 `movies.json`에 없어서 각각의 `숫자.json`을 불러와서 정보를 살펴보라고 하였는데?
>
> 실제로 movies.json을 보면 release_date가 존재한다..
>
> `숫자.json` 파일에 있는` release_date`와 다르다, 이건 뭘 의미하는 것일까?
>
> 
>
> 문제 풀이에 있어서 어려움을 끼치는 것은 아니기 때문에 순수한 궁금증이었다.





## 결론

> 해당 문제들 중 특히 d,e 문제를 풀면서 들었던 생각은
>
> 
>
> "이렇게 json 파일을 for문으로 반복하며 하나하나 각각 불러와서 key에 따른 value 값을 호출한 다음, 조건문을 사용하는 것이 가장 빠른 방법이 맞는걸까?"
>
> 하는 생각이 들었다..



> json 파일들을 전부다 합쳐서 파일을 만들고 (movies.json와 비슷한 형태로) 이를 불러옴으로써 하나의 데이터에 대해서 for문을 돌면서 조건문을 반복하는게 좋지 않을까?
>
> 라는 생각 말이다.
>
> 
>
> 하지만, 어차피 for문으로 다 불러오고 이를 하나의 데이터로 합치고 한 다음, 또 다시 for문으로 루프를 돌리며 조건문을 수행한다는게
>
> 1. 시간(for loop의 추가),
> 2. 메모리 연산량(더 큰 파일을 불러온 다음 작업해야 한다는 점)
>
> 측면에서 손해일 거라는 생각이 분명히 들었다.



- 실제로 현업데이터 혹은 직접 크롤링한 데이터를 다루게 된다면 이보다 더 복잡하고 어려운 데이터를 만나게 될 것이다.
- 보다 인덱싱과 key 호출 등에 익숙해지기 위해 노력하며, 무리없이 더 복잡한 데이터를 잘 다뤄야겠다는 결심이 들었다.

